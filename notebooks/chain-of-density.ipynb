{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import math\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_density(sentence:str):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    entities = nlp(sentence).ents\n",
    "    entity_density = round(len(entities)/len(tokens),3)\n",
    "\n",
    "    return len(tokens),len(entities),entity_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field,field_validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n",
    "    yet highly non-specific, containing little information beyond the entities marked as missing.\n",
    "    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n",
    "    \"\"\"\n",
    "\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n",
    "        It should be roughly 80 words in length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewrittenSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    This is a new, denser summary of identical length which covers every entity\n",
    "    and detail from the previous summary plus the Missing Entities.\n",
    "\n",
    "    Guidelines\n",
    "    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n",
    "    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n",
    "    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n",
    "    - Missing entities can appear anywhere in the new summary\n",
    "\n",
    "    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n",
    "    \"\"\"\n",
    "\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n",
    "    )\n",
    "    absent: List[str] = Field(\n",
    "        ...,\n",
    "        default_factory=list,\n",
    "        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n",
    "    )\n",
    "    missing: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n",
    "    )\n",
    "        \n",
    "    \n",
    "    # @field_validator(\"summary\")\n",
    "    # def min_length(cls, v: str):\n",
    "    #     tokens = nltk.word_tokenize(v) \n",
    "    #     num_tokens = len(tokens)\n",
    "    #     if num_tokens < 60:\n",
    "    #         raise ValueError(\n",
    "    #             \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n",
    "    #         )\n",
    "    #     return v\n",
    "    \n",
    "    # @field_validator(\"missing\")\n",
    "    # def has_missing_entities(cls, missing_entities: List[str]):\n",
    "    #     if len(missing_entities) == 0:\n",
    "    #         raise ValueError(\n",
    "    #             \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n",
    "    #         )\n",
    "    #     return missing_entities\n",
    "    \n",
    "    # @field_validator(\"absent\")\n",
    "    # def has_no_absent_entities(cls, absent_entities: List[str]):\n",
    "    #     absent_entity_string = \",\".join(absent_entities)\n",
    "    #     if len(absent_entities) > 0:\n",
    "    #         print(f\"Detected absent entities of {absent_entity_string}\")\n",
    "    #         raise ValueError(\n",
    "    #             f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n",
    "    #         )\n",
    "    #     return absent_entities\n",
    "    \n",
    "    # @field_validator(\"summary\")\n",
    "    # def min_entity_density(cls, v: str):\n",
    "    #     tokens = nltk.word_tokenize(v)\n",
    "    #     num_tokens = len(tokens)\n",
    "    \n",
    "    #     # Extract Entities\n",
    "    #     doc = nlp(v) \n",
    "    #     num_entities = len(doc.ents)\n",
    "    \n",
    "    #     density = num_entities / num_tokens\n",
    "    #     if density < 0.08: \n",
    "    #         raise ValueError(\n",
    "    #             f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n",
    "    #         )\n",
    "    \n",
    "    #     return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = instructor.patch(OpenAI()) \n",
    "\n",
    "def summarize_article(article: str, summary_steps: int = 3):\n",
    "    summary_chain = []\n",
    "    # We first generate an initial summary\n",
    "    summary: InitialSummary = client.chat.completions.create(  \n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        response_model=InitialSummary,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"The generated summary should be about 80 words.\",\n",
    "            },\n",
    "        ],\n",
    "        max_retries=4,\n",
    "    )\n",
    "    prev_summary = None\n",
    "    summary_chain.append(summary.summary)\n",
    "    for i in range(summary_steps):\n",
    "        missing_entity_message = (\n",
    "            []\n",
    "            if prev_summary is None\n",
    "            else [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        new_summary: RewrittenSummary = client.chat.completions.create( \n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                You are going to generate an increasingly concise,entity-dense summary of the following article.\n",
    "\n",
    "                Perform the following two tasks\n",
    "                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n",
    "                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n",
    "\n",
    "                Guidelines\n",
    "                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n",
    "                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n",
    "                - Missing entities can appear anywhere in the new summary\n",
    "                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "                \"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n",
    "                },\n",
    "                *missing_entity_message,\n",
    "            ],\n",
    "            max_retries=4, \n",
    "            max_tokens=1000,\n",
    "            response_model=RewrittenSummary,\n",
    "        )\n",
    "        summary_chain.append(new_summary.summary)\n",
    "        prev_summary = new_summary\n",
    "\n",
    "    return summary_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./data/forward_citations.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.head(1)['Complete Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarize_article(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 -> Results (Tokens: 78, Entity Count: 2, Density: 0.026)\n",
      "Article 2 -> Results (Tokens: 87, Entity Count: 10, Density: 0.115)\n",
      "Article 3 -> Results (Tokens: 151, Entity Count: 15, Density: 0.099)\n",
      "Article 4 -> Results (Tokens: 129, Entity Count: 12, Density: 0.093)\n"
     ]
    }
   ],
   "source": [
    "for index,summary in enumerate(summaries):\n",
    "    tokens,entity,density = calculate_entity_density(summary)\n",
    "    print(f\"Article {index+1} -> Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "In an exposition marked by an interstitial navigational framework, the article meticulously articulates the proceedings surrounding a legal dispute entailing the Recording Industry Association of America, Inc. and Verizon Internet Services, Inc., contesting the use of certain legislative mandates. This discourse delineates the intricacies accompanying the issuance of subpoenas within the context of internet-enabled copyright infringements, culminating in a judicial pronouncement that reverberates through the substratum of digital information exchange jurisprudence.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "The article examines a U.S. Court of Appeals case, RIAA v. Verizon Internet Services, where the RIAA's subpoenas for user information from Verizon under the Digital Millennium Copyright Act's (DMCA) provision were disputed. The Court decided the DMCA does not permit subpoenas to an ISP simply acting as a conduit for user communications, thus Verizon was not obliged to reveal its subscribers' identities. The decision has broader implications for copyright law enforcement in the digital age.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "The U.S. Court of Appeals reviewed RIAA v. Verizon Internet Services, adjudicating whether the RIAA could subpoena Verizon for customer data under DMCA's 17 U.S.C. \n",
       "512(h) relating to online copyright infringement. The Court held that DMCA's provision does not authorize subpoenas to ISPs like Verizon acting merely as conduits, thus negating Verizon's obligation to reveal subscriber identities. This interpretation is reinforced by the statute's structure and definitions in Section 512(k)(1)(B), and the scope of safe harbor provisions. The adjudication took into account rules such as Federal Rule of Civil Procedure 45(c)(2)(B), implicating constitutional considerations for anonymous speech and association. The verdict shapes the enforcement of copyright law in the context of peer-to-peer file-sharing and internet privacy.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "The U.S. Court of Appeals in RIAA v. Verizon Internet Services examined the DMCA's subpoena power under 17 U.S.C. \n",
       "512(h) and concluded ISPs acting as mere conduits like Verizon aren't obliged to disclose user identities. The Court's decision relied on the statutory construction of Section 512, specific limitations within Section 512(k)(1)(B), and the parameters of safe harbor provisions. Additionally, the ruling took into account Federal Rule of Civil Procedure 45(c)(2)(B), with implications for constitutional rights to anonymous speech and association. The outcome affects copyright law application and internet user privacy in the scenario of peer-to-peer file-sharing.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for summary in summaries:\n",
    "    display(Markdown(f\"\\n{summary}\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
